{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating Predictor Using Logistic Regression, Bayes and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Import functions from py files\n",
    "\n",
    "from models import run_all_models, cross_validation\n",
    "from import_and_split import read_data, split_and_vectorise\n",
    "from sampling import over_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data and splitting with imported function\n",
    "df = read_data()\n",
    "X_train, X_test, y_train, y_test = split_and_vectorise(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data has been explored and cleaned using cleaning.py file previously. Please refer to this file to see the cleaning methods used. The df was a dataframe of amazon reviews and ratings. The reviews has been cleaned following these steps:\n",
    "- Lowercasing\n",
    "- Remove stopwords\n",
    "- Stem\n",
    "- Tokenised\n",
    "\n",
    "Then the data has been split up into train and test data, as well as vectorised. This means the words have been split up into seperate columns and represented in binary, with max features being set as 5000. \"TfidfVectorizer\" was used in this instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression, Bayes and Random Forest Modelling Results\n",
    "\n",
    "### Test Stage 1\n",
    "\n",
    "Please see models.py for functions used for these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the run_all_models function from models.py, returning all model results\n",
    "model_results = run_all_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting each models test results\n",
    "logistic_reg, bayes, random_forest = model_results[0], model_results[1], model_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logstic Reg: 85.71%\n",
      "Accuracy for Bayes: 39.97%\n",
      "Accuracy for Random Forest: 85.57%\n"
     ]
    }
   ],
   "source": [
    "#Print accuracy as a percentage\n",
    "print(\"Accuracy for Logstic Reg: {}%\".format((logistic_reg * 100).round(2)))\n",
    "print(\"Accuracy for Bayes: {}%\".format((bayes * 100).round(2)))\n",
    "print(\"Accuracy for Random Forest: {}%\".format((random_forest * 100).round(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    2980\n",
       "4.0     270\n",
       "3.0      92\n",
       "1.0      66\n",
       "2.0      54\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing the unbalanced dataset\n",
    "df[\"rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression and random forest produced similar results in this first test stage. Bayes performed very poorly, which can be caused by overfitting the training set. As the values above suggest, there is a heavy weight towards 5 star rating reviews. I will now look into oversampling the data, to try and increase the amount of lower rated reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Stage 2\n",
    "### Oversampling Results For All Models\n",
    "\n",
    "Please see models.py and sampling.py for functions used for these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the models using over sampling now\n",
    "df_over_sample = over_sampling(df)\n",
    "over_sample_results = run_all_models(df_over_sample[0], df_over_sample[1], df_over_sample[2], df_over_sample[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_resample, bayes_resample, forest_resample = over_sample_results[0], over_sample_results[1], over_sample_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logstic Reg: 86.58%\n",
      "Accuracy for Bayes: 41.41%\n",
      "Accuracy for Random Forest: 84.56%\n"
     ]
    }
   ],
   "source": [
    "#Print accuracy as a percentage\n",
    "print(\"Accuracy for Logstic Reg: {}%\".format((log_resample * 100).round(2)))\n",
    "print(\"Accuracy for Bayes: {}%\".format((bayes_resample * 100).round(2)))\n",
    "print(\"Accuracy for Random Forest: {}%\".format((forest_resample * 100).round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for all models have a very small improvement. The 5 star reviews weren't undersampled, which may be a reason for only seeing a small improvement in performance. Now, I'll look into using cross-validation to see if the model results can be improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Stage 3\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "Please see models.py for functions used for these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = cross_validation(df[\"test\"], df[\"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cv, bayes_cv, forest_cv = cross_val[0], cross_val[1], cross_val[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logstic Reg: 86.19%\n",
      "Accuracy for Bayes: 85.93%\n",
      "Accuracy for Random Forest: 39.33%\n"
     ]
    }
   ],
   "source": [
    "#Print accuracy as a percentage\n",
    "log_cv_result, bayes_cv_result, forest_cv_result = log_cv.round(4), bayes_cv.round(4), forest_cv.round(4)\n",
    "print(\"Accuracy for Logstic Reg: {}%\".format(log_cv_result * 100))\n",
    "print(\"Accuracy for Bayes: {}%\".format((bayes_cv_result * 100).round(2)))\n",
    "print(\"Accuracy for Random Forest: {}%\".format(forest_cv_result * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression looks like it performed best overall and was consistant throughout all testing. Cross Validation improved Bayes, which performed badly over the first 2 test stages. Random Forest had similar results over the first two test stages but performed poorly in cross validation.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
